#!/usr/bin/env python3
"""
Async health check for novel sites.
- Concurrency limited (default 20)
- Supports per-site one or multiple test URLs
- Skips duplicate issue creation
- Designed to run in CI on a schedule

Environment (in CI):
  GITHUB_TOKEN       -> GitHub token
  GITHUB_REPOSITORY  -> "owner/repo"
"""

from __future__ import annotations

import asyncio
import json
import os
from collections.abc import Iterable, Mapping
from dataclasses import dataclass
from datetime import UTC, datetime
from typing import Any

import aiohttp

# ----------------------------
# Configuration
# ----------------------------

CONFIG_PATH = "scripts/site_health_config.json"
DEFAULT_MAX_CONCURRENT = 20
DEFAULT_TIMEOUT_SECS = 12
DEFAULT_RETRIES = 1

GITHUB_API = "https://api.github.com"
LABEL_SITE_HEALTH = "site-health"
LABEL_UNREACHABLE = "unreachable"

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/134.0.0.0 Safari/537.36"
)
USER_HEADERS = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",  # noqa: E501
    "Accept-Encoding": "gzip, deflate",
    "Accept-Language": "en,zh;q=0.9,zh-CN;q=0.8",
    "User-Agent": USER_AGENT,
    "Connection": "keep-alive",
}

_DEFAULT_TIMEOUT = aiohttp.ClientTimeout(total=30)


@dataclass
class SiteResult:
    site: str
    ok: bool
    message: str
    url: str


def _utc_now_iso() -> str:
    return datetime.now(UTC).isoformat()


def normalize_sites(obj: Mapping[str, Any]) -> dict[str, list[str]]:
    out: dict[str, list[str]] = {}
    for k, v in obj.items():
        if isinstance(v, str):
            out[k] = [v]
        elif isinstance(v, Iterable):
            out[k] = [str(x) for x in v]
        else:
            raise TypeError(f"Invalid URL mapping for site {k!r}: {type(v).__name__}")
    return out


def load_config(path: str = CONFIG_PATH) -> dict[str, Any]:
    """Load config from JSON file; fallback to defaults."""
    if not os.path.exists(path):
        print(f"Config file not found: {path}. Using defaults.")
        return {
            "max_concurrent": DEFAULT_MAX_CONCURRENT,
            "timeout": DEFAULT_TIMEOUT_SECS,
            "retries": DEFAULT_RETRIES,
            "sites": {},
        }
    with open(path, encoding="utf-8") as f:
        data = json.load(f)
    return data


# ----------------------------
# GitHub helpers
# ----------------------------


def _gh_headers(token: str) -> dict[str, str]:
    return {
        "Authorization": f"Bearer {token}",
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28",
        "User-Agent": "novel-downloader-health-check",
    }


async def ensure_labels_exist(
    session: aiohttp.ClientSession, token: str, repo: str
) -> None:
    desired = {
        LABEL_SITE_HEALTH: {"color": "1d76db", "description": "Site health monitoring"},
        LABEL_UNREACHABLE: {"color": "d73a4a", "description": "Unreachable site"},
    }
    headers = _gh_headers(token)
    for name, meta in desired.items():
        url = f"{GITHUB_API}/repos/{repo}/labels/{name}"
        async with session.get(url, headers=headers, timeout=_DEFAULT_TIMEOUT) as r:
            if r.status == 404:
                async with session.post(
                    f"{GITHUB_API}/repos/{repo}/labels",
                    headers=headers,
                    json={"name": name, **meta},
                    timeout=_DEFAULT_TIMEOUT,
                ):
                    pass


async def fetch_open_health_issues(
    session: aiohttp.ClientSession, token: str, repo: str
) -> dict[str, int]:
    headers = _gh_headers(token)
    params = {"state": "open", "labels": LABEL_SITE_HEALTH, "per_page": "100"}
    url = f"{GITHUB_API}/repos/{repo}/issues"
    async with session.get(
        url, headers=headers, params=params, timeout=_DEFAULT_TIMEOUT
    ) as r:
        if r.status >= 400:
            text = await r.text()
            raise RuntimeError(f"GitHub issues GET failed: {r.status} {text}")
        issues = await r.json()
    result: dict[str, int] = {}
    for issue in issues:
        title = str(issue.get("title", ""))
        if title.startswith("site(") and "): unreachable" in title:
            site = title.split("(", 1)[1].split(")", 1)[0]
            result[site] = int(issue.get("number"))
    return result


async def create_issue(
    session: aiohttp.ClientSession, token: str, repo: str, site: str, message: str
) -> str | None:
    headers = _gh_headers(token)
    title = f"site({site}): unreachable"
    body = (
        f"Site `{site}` appears unreachable.\n\n"
        f"- Error: `{message}`\n"
        f"- Checked at: {_utc_now_iso()}\n\n"
        "_This issue was automatically generated by the site health check script._"
    )
    async with session.post(
        f"{GITHUB_API}/repos/{repo}/issues",
        headers=headers,
        json={
            "title": title,
            "body": body,
            "labels": [LABEL_SITE_HEALTH, LABEL_UNREACHABLE],
        },
        timeout=_DEFAULT_TIMEOUT,
    ) as r:
        if r.status >= 400:
            text = await r.text()
            print(f"Failed to create issue for {site}: {r.status} {text}")
            return None
        data = await r.json()
    issue_url = data.get("html_url")
    print(f"Created issue for {site}: {issue_url}")
    return str(issue_url) if issue_url else None


# ----------------------------
# HTTP check
# ----------------------------


async def _fetch_ok(
    session: aiohttp.ClientSession, url: str, timeout: aiohttp.ClientTimeout
) -> tuple[bool, str]:
    try:
        async with session.get(url, timeout=timeout) as resp:
            if 200 <= resp.status < 400:
                return True, str(resp.status)
            return False, f"HTTP {resp.status}"
    except Exception as e:
        return False, f"{type(e).__name__}: {e}"


async def check_one_site(
    session: aiohttp.ClientSession,
    site: str,
    urls: list[str],
    timeout_secs: int,
    retries: int,
    sem: asyncio.Semaphore,
) -> SiteResult:
    timeout = aiohttp.ClientTimeout(total=timeout_secs)
    last_msg = "no urls"
    for url in urls:
        for attempt in range(retries + 1):
            async with sem:
                ok, msg = await _fetch_ok(session, url, timeout)
            if ok:
                return SiteResult(site=site, ok=True, message=msg, url=url)
            last_msg = msg
            if attempt < retries:
                await asyncio.sleep(0.5)
    return SiteResult(
        site=site, ok=False, message=last_msg, url=urls[0] if urls else ""
    )


async def run_checks(
    sites: dict[str, list[str]], max_concurrent: int, timeout_secs: int, retries: int
) -> list[SiteResult]:
    sem = asyncio.Semaphore(max_concurrent)
    connector = aiohttp.TCPConnector(ssl=False, limit_per_host=max_concurrent)
    async with aiohttp.ClientSession(
        headers=USER_HEADERS, connector=connector
    ) as session:
        tasks = [
            check_one_site(session, site, urls, timeout_secs, retries, sem)
            for site, urls in sites.items()
        ]
        return await asyncio.gather(*tasks)


# ----------------------------
# Main
# ----------------------------


async def async_main() -> int:
    config = load_config()
    sites = normalize_sites(config.get("sites", {}))
    if not sites:
        print("No sites configured.")
        return 0

    results = await run_checks(
        sites,
        config.get("max_concurrent", DEFAULT_MAX_CONCURRENT),
        config.get("timeout", DEFAULT_TIMEOUT_SECS),
        config.get("retries", DEFAULT_RETRIES),
    )

    failed = {r.site: r.message for r in results if not r.ok}
    ok_sites = {r.site for r in results if r.ok}

    print("\n=== Site Health Summary ===")
    for r in sorted(results, key=lambda x: x.site):
        status = "OK" if r.ok else "FAIL"
        print(f"[{status}] {r.site:15s} -> {r.message:>8s} ({r.url})")

    token = os.getenv("GITHUB_TOKEN")
    repo = os.getenv("GITHUB_REPOSITORY")
    if not token or not repo:
        print(
            "Missing GITHUB_TOKEN or GITHUB_REPOSITORY; skipping GitHub issue creation."
        )
        return 1 if failed else 0

    async with aiohttp.ClientSession() as gh_session:
        await ensure_labels_exist(gh_session, token, repo)
        open_issues = await fetch_open_health_issues(gh_session, token, repo)

        for site, message in failed.items():
            if site in open_issues:
                print(f"{site}: open issue already exists (#{open_issues[site]})")
                continue
            await create_issue(gh_session, token, repo, site, message)

        recovered = [s for s in ok_sites if s in open_issues]
        if recovered:
            print("\nRecovered sites (please close manually):")
            print(", ".join(sorted(recovered)))

    return 1 if failed else 0


def main() -> None:
    raise SystemExit(asyncio.run(async_main()))


if __name__ == "__main__":
    main()
